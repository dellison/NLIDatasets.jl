<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · NLIDatasets.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>NLIDatasets.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Home</a><ul class="internal"><li><a class="toctext" href="#SNLI-1">SNLI</a></li><li><a class="toctext" href="#MultiNLI-1">MultiNLI</a></li><li><a class="toctext" href="#XNLI-1">XNLI</a></li><li><a class="toctext" href="#HANS-1">HANS</a></li><li><a class="toctext" href="#BreakingNLI-1">BreakingNLI</a></li><li><a class="toctext" href="#SciTail-1">SciTail</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Home</a></li></ul><a class="edit-page" href="https://github.com/dellison/NLIDatasets.jl/blob/master/docs/src/index.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Home</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="NLIDatasets.jl-1" href="#NLIDatasets.jl-1">NLIDatasets.jl</a></h1><p>NLIDatasets.jl is a Julia package for working with datasets for the Natural Language Inference task (also called Relational Text Entailment, or RTE).</p><p>Provides an interface to the following datasets:</p><h2><a class="nav-anchor" id="SNLI-1" href="#SNLI-1">SNLI</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLIDatasets.SNLI" href="#NLIDatasets.SNLI"><code>NLIDatasets.SNLI</code></a> — <span class="docstring-category">Module</span>.</div><div><div><pre><code class="language-julia">SNLI</code></pre><p>A corpus of 570k human-written English sentence pairs for NLI.</p><p>SNLI sentence pairs are manually labeled with labels entailment, contradiction, and neutral.</p><p>For details, see the <a href="https://nlp.stanford.edu/projects/snli/">SNLI home page</a> or read the <a href="https://nlp.stanford.edu/pubs/snli_paper.pdf">2015 paper</a> &quot;A large annotated corpus for learning natural language inference&quot; by Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher Manning.</p><p>Included data:</p><pre><code class="language-julia">SNLI.train_tsv()
SNLI.train_jsonl()
SNLI.dev_tsv()
SNLI.dev_jsonl()
SNLI.test_tsv()
SNLI.test_jsonl()</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/dellison/NLIDatasets.jl/blob/bca0ab5652e4a19b9503f638717660cfec3eb288/src/snli.jl#L1-L22">source</a></section><h2><a class="nav-anchor" id="MultiNLI-1" href="#MultiNLI-1">MultiNLI</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLIDatasets.MultiNLI" href="#NLIDatasets.MultiNLI"><code>NLIDatasets.MultiNLI</code></a> — <span class="docstring-category">Module</span>.</div><div><div><pre><code class="language-julia">MultiNLI</code></pre><p>A corpus of 433k sentence pairs for NLI.</p><p>For details, see the <a href="https://www.nyu.edu/projects/bowman/multinli/">MultiNLI home page</a> or read the <a href="https://www.nyu.edu/projects/bowman/multinli/paper.pdf">2018 paper</a> &quot;A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference&quot; by Adina Williams, NIkita Nangia, and Samuel R. Bowman.</p><p>Included data:</p><pre><code class="language-julia">MultiNLI.train_tsv()
MultiNLI.train_jsonl()
MultiNLI.dev_matched_tsv()
MultiNLI.dev_matched_jsonl()
MultiNLI.dev_mismatched_tsv()
MultiNLI.dev_mismatched_jsonl()</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/dellison/NLIDatasets.jl/blob/bca0ab5652e4a19b9503f638717660cfec3eb288/src/multinli.jl#L1-L18">source</a></section><h2><a class="nav-anchor" id="XNLI-1" href="#XNLI-1">XNLI</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLIDatasets.XNLI" href="#NLIDatasets.XNLI"><code>NLIDatasets.XNLI</code></a> — <span class="docstring-category">Module</span>.</div><div><div><pre><code class="language-julia">XNLI</code></pre><p>A collection of 5,000 test and 2,500 dev pairs for the <code>MultiNLI</code> corpus.</p><p>For details, see the <a href="https://www.aclweb.org/anthology/papers/D/D18/D18-1269/">2018 paper</a> &quot;XNLI: Evaluating Cross-lingual Sentence Representations.&quot;</p><pre><code class="language-julia">XNLI.dev_tsv()
XNLI.dev_jsonl()
XNLI.test_tsv()
XNLI.test_jsonl()</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/dellison/NLIDatasets.jl/blob/bca0ab5652e4a19b9503f638717660cfec3eb288/src/xnli.jl#L1-L14">source</a></section><h2><a class="nav-anchor" id="HANS-1" href="#HANS-1">HANS</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLIDatasets.HANS" href="#NLIDatasets.HANS"><code>NLIDatasets.HANS</code></a> — <span class="docstring-category">Module</span>.</div><div><div><pre><code class="language-julia">HANS</code></pre><p>HANS (Heauristic Analysis for NLI Systems) is a dataset for NLI.</p><p>It contains the set of examples used in the 2019 paper &quot;Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference&quot; by R. Tom McCoy, Ellie Pavlick, and Tal Linzen.</p><p>For details, see the <a href="https://www.aclweb.org/anthology/P19-1334">2019 paper</a> &quot;Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference&quot; by by R. Tom McCoy, Ellie Pavlick, and Tal Linzen.</p><p>Consists of a set of examples for evaluation, provided with <code>test_tsv</code>.</p><pre><code class="language-julia">HANS.test_tsv()</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/dellison/NLIDatasets.jl/blob/bca0ab5652e4a19b9503f638717660cfec3eb288/src/hans.jl#L1-L19">source</a></section><h2><a class="nav-anchor" id="BreakingNLI-1" href="#BreakingNLI-1">BreakingNLI</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLIDatasets.BreakingNLI" href="#NLIDatasets.BreakingNLI"><code>NLIDatasets.BreakingNLI</code></a> — <span class="docstring-category">Module</span>.</div><div><div><pre><code class="language-julia">BreakingNLI</code></pre><p>A dataset of 8193 premise-hypothesis sentence-pairs for NLI.</p><p>Each example is annotated to entailment, contradiction, and neutral. The premise and the hypothesis are identical except for one word/phrase that was replaced. This dataset is meant for testing methods trained to solve the natural language inference task, and it requires some lexical and world knowledge to achieve reasonable performance on it.</p><p>For details, see the <a href="https://github.com/BIU-NLP/Breaking_NLI">GitHub page</a> or read the <a href="https://aclweb.org/anthology/P18-2103/">2018 paper</a>.</p><p>Available data:</p><pre><code class="language-julia">BreakingNLI.test_jsonl()</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/dellison/NLIDatasets.jl/blob/bca0ab5652e4a19b9503f638717660cfec3eb288/src/breaking_nli.jl#L1-L19">source</a></section><h2><a class="nav-anchor" id="SciTail-1" href="#SciTail-1">SciTail</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NLIDatasets.SciTail" href="#NLIDatasets.SciTail"><code>NLIDatasets.SciTail</code></a> — <span class="docstring-category">Module</span>.</div><div><div><pre><code class="language-julia">SciTail</code></pre><p>SciTail is a NLI dataset created from multiple-choice science exams and web sentences.</p><p>For details, see the <a href="http://ai2-website.s3.amazonaws.com/publications/scitail-aaai-2018_cameraready.pdf">2018 paper</a> &quot;SciTail: A Textual Entailment Dataset from Science Question Answering&quot; by Tushar Khot, Asish Sabharwal, and Peter Clark.</p><pre><code class="language-julia">SciTail.train_tsv()
SciTail.train_jsonl()
SciTail.dev_tsv()
SciTail.dev_jsonl()
SciTail.test_tsv()
SciTail.test_jsonl()</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/dellison/NLIDatasets.jl/blob/bca0ab5652e4a19b9503f638717660cfec3eb288/src/scitail.jl#L1-L17">source</a></section><footer><hr/></footer></article></body></html>
